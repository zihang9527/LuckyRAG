In this report, we present a series of math-specific large language models:
Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of
the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During
the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale,
high-quality mathematical data. (2) In the post-training phase, we develop a reward
model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This
RM is then applied to the iterative evolution of data in supervised fine-tuning
(SFT). With a stronger SFT model, it’s possible to iteratively train and update the
RM, which in turn guides the next round of SFT data iteration. On the final SFT
model, we employ the ultimate RM for reinforcement learning, resulting in the
Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is
used to guide sampling, optimizing the model’s performance.
Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced
mathematical reasoning capabilities, including Chain-of-Thought (CoT) and ToolIntegrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets
in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and
AIME24, covering a range of difficulties from grade school level to math competition problems. The flagship model, Qwen2.5-Math-72B-Instruct, significantly
outperforms both open-source models and leading closed-source models (e.g., GPT4o, Gemini Math-Specialized 1.5 Pro). Particularly in the challenging AMC 2023,
with the assistance of RM, Qwen2.5-Math-72B-Instruct successfully solves almost
all the problems. Qwen2.5-Math-7B-Instruct surpasses Qwen2-Math-Instruct 72B
in performance. Under CoT and TIR settings, it achieves MATH scores of 83.6
and 85.3, respectively. Even our smallest 1.5B model, achieving a MATH score
of around 80 when utilizing the Python Interpreter, outperforms the majority of
current models in this domain. We hope that Qwen2.5-Math can contribute to the
community for solving complex mathematical problems.
The base models, instruct models, and reward model of the Qwen2.5-Math series
are available on Hugging Face 1
and ModelScope2
, and the evaluation scripts on
GitHub3
. We have also developed a demo that supports the TIR mode in QwenAgent4
, which allows running code locally to experience Tool-Integrated Reasoning
capabilities of Qwen2.5-Math.

Over the past year, we have devoted considerable effort to researching and enhancing the reasoning
capabilities of large language models, with a particular emphasis on their ability to solve arithmetic
and mathematical problems. In this report, we introduce a series of math-specific large language
models, Qwen2.5-Math, Qwen2.5-Math-RM, and Qwen2.5-Math-Instruct-1.5B/7B/72B. To provide
a comprehensive understanding of the technical developments behind Qwen2.5-Math, we also offer a
detailed overview of its predecessor, Qwen2-Math (Qwen, 2024).
We introduce a series of self-improvement techniques to develop Qwen2.5-Math models on top of
the Qwen2-Math. Self-improvement techniques take advantage of supervision from large language
models themselves (Cao et al., 2024). Specifically, we apply self-improvement from three aspects
during the training of Qwen2.5-Math. In pre-training, we employ Qwen2-Math-Instruct to synthesize
math queries and corresponding responses on a large scale to enrich the pre-training corpus of
Qwen2.5-Math. In post-training, we train a reward model on massive sampling from previous models
and apply it to the iterative evolution of data in supervised fine-tuning. The better mathematical
models trained from this enhancement lead to a more robust reward model, Qwen2.5-Math-RM.
Then, we use this reward model in reinforcement learning and best-of-N sampling during inference.
Synthetic data and judgment play a significant role in the enhancement of Qwen2.5-Math compared
with its predecessor.
Specifically, the overall pipelines for developing Qwen2-Math and Qwen2.5-Math are illustrated in
Figure 2. First, the Qwen2-Math base models are trained on a high-quality mathematical pre-training
dataset called the Qwen Math Corpus v1, which contains approximately 700 billion tokens. Second,
we train a math-specific reward model Qwen2-Math-RM, derived from Qwen2-Math-72B, to create
the Qwen2-Math-Instruct models. This reward model is used to construct Supervised Fine-Tuning
(SFT) data through Rejection Sampling (Yuan et al., 2023). Moreover, the reward model plays a
key role in the reinforcement learning stage, where we employ Group Relative Policy Optimization
(GRPO) (Shao et al., 2024) following SFT. Third, leveraging the Qwen2-Math-72B-Instruct model,
we synthesize additional high-quality mathematical pre-training data, which serves as the foundation
for Qwen Math Corpus v2. This updated corpus contains over 1 trillion tokens and is used to pre-train
the Qwen2.5-Math models. Lastly, similar to the process used for the Qwen2-Math-Instruct models,
we construct the Qwen2.5-Math-RM and Qwen2.5-Math-Instruct models. An important distinction
in this stage is the inclusion of both English and Chinese Chain-of-Thought (CoT) reasoning data, as
